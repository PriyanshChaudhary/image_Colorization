# image_Colorization
In earlier times, converting a black and white image into a vibrant and acceptable modification was done by using survey and massive user feedback and interaction to produce a convincing output. However, the ensuing result turned out to be a unsaturated version of the what the real-life version of the image depicted. Here in this venture, the aim was to target the underlying issue of the problem which causes these unsaturated and non-vibrant colorization using Machine Learning. The complication here is handled by issuing the latent problem in the form of multiple classes and re-balancing the colors at the time when during training to increase the scope of the usage of varied colors in the results.
1. INTRODUCTION:
The process of colorizing the grayscale images is possible by thinking it in relation to what our mind is capable of thinking and its capability to imagine. Our consciousness perceives the leaves as green and the sky as blue in colour. This perception of our mind is what allows us to create a base for this colorization model. This is the reason why the neural network is so named as it is based on the neurons in our brain for thinking and altering our perspective. The complications we have to face in creating this neural algorithm is what we need to work out on in order to realise its importance in fields involving graphic visualization. My purpose in this was to make a tunnel path with the help of neural structures to help in the case of image recognition and object identification.
The kind of approach applied in this project is mainly for making an automatic system which takes a given input and provide a satisfactory analysis better than the past attempts that have been made in this field. Making an attempt at colorizing the grayscale images will escalate their uses in variety of domains such as improvising the efficiency and capabilities of surveillance equipment, perfecting old age photographs and such. The visual data that is provided by a grayscale image is limited and does not give a perfect overview of its semantics. Hence, colorizing grayscale images provide more insights about its properties by adding colour components. Thus, with the help of machine learning models which we train using datasets of colourful images, we attempt on improvising these results. Until recently people have used the method of photoshopping black and white images and we could see that this process could take a month or more, however, with this model we aim to automate this task and make it more reliable and effective for the consequent purposes if our aim is ultimately being to be at par of human eye.
With this purpose, I devised this model which to a certain extent is able to perform colorization of images by utilising the help of a Convolutional Neural Network architecture and datasets from different repositories, I was able to build a CNN trained from the beginning. Also, a decoder -encoder program plus an extractor for reading the image specializations was required in implementing the steps for this project to come out. Due to multiple hardware and time constraints, this model was only accepting a restricted setting of images as input and labels which makes this model underperform a little but also signifies on its potential if it was made into further study. Nevertheless, this report will give you the complete idea of the approach I
2
made into building this model which can help us automate the idea of automatic image colorization.
1.1 Artificial Neural Network
Before that let’s start with the topic of neural network at start first. What it is? What does it mean? Where it is used? And at exactly what phase and for what purpose do we implement them in our program here.
Artificial Neural Networks or (ANNs) as we define them are a cluster of pipelines and connections which are made to LEARN a particular set of actions without the need for any continuous programming or set of codes for each different iteration. This makes them quite similar in structure and performance to the neurons we have in our brain. These neurons are those body parts in our brain which take the visual data as input from what our eyes see, interpret it and then send electrical impulses in our brain to perform task that helps to objectify or recognise the pattern we are currently observing. An ANN is based o similar structures called nodes which are units
That are in connection with other nodes and like the synapses which happen in our brain can transmit a signal in between other artificial neuron. However, unlike the electrical impulses which are transmitted in our brain, the artificial neurons take a real number as an input and processes it with the help of other non-linear functions in algorithms and compute themselves an output. The pipelines that connect the artificial neurons with each other are called edges and have a number called weight which is assigned to each one of the edges. As humans, we all have a subconscious ability to give priorities to tasks if we have a multiple number of them at a single time. This same concept is applied in a neural network that these weights are used in computing the priority of certain tasks that the neural network is set to perform. These priorities are said to be essential in determining the actions that the neural network will categorise more importantly than the rest of the installed tasks. In the end there is a threshold number in the final stage and the signal is transmitted if the aggregate value does surpass the threshold. This is how the neural network creates a relationship between the input and output layer with the help of the system. In between this system we also have multiple hidden layers that perform varied set of operations with multiple kinds of input. The purpose because of which the ANN came into picture was that we needed a machine to solve problems by applying a similar approach to how a human brain would act in that particular situation. However, not limiting
3
themselves to machine translations, they have proven themselves valid in multiple fields of computer vision, image/speech recognition, social network filtering and medical diagnosis.
In this case, we come across a term referred to as Perceptron. A multilayer perceptron us a class of feed-forward artificial neural network. An MLP composes of a minimum threshold of three node layers.
1.2 Convolutional Neural Network (CNN)
Within the field of deep learning, a CNN or ConvNet as it is commonly referred to, is a class of deep non-cyclic artificial neural network which find its extensive use in analysing visual imagery.
It uses multiple phases of multilayer perceptrons and is built on the basis to work with minimal pre-processing. The CNN is different from other versions of artificial neural network as it utilises very few amounts of pre-processing relative to other image classification algorithms. It holds an advantage in the fact that it automates the filters itself which in previous versions were hand-built.
So how does this help me? CNN architecture runs on a pre-assumed task that it recognises the input as images, that in later phases is beneficial to us as we can set up some characters and features into the architecture that required another set of datasets earlier. Thus, the non-linearity of the functions become more efficient and the values to be provided are computed by themselves.
1.3 TensorFlow
It is an open source library for a very high-performance numerical computation. It has a flexible architecture, and it provides easy deployment of calculation across different platforms. It was initially developed by engineers and researchers from the Google Brain team. It provides sturdy support for deep learning and machine learning.
4
2. Methodology:
CNN’s ability to recognise inputs as images makes us to improvise the architecture in a more prudent manner. Unlike a Regular Neural Network, a CNN basically comprises of neurons arranged with height, depth and width as its three parameters. Here is a depiction below:
Neural Network and a ConvNet with its arrangement of neurons in 3D (width, depth, height), as depicted in one of the layers
2.1 Core logic
My first step was that of image rendering and how to put a layer of digital colours into the picture. Well this was the main logic for the neural network that I had to use.
So, from theory we know that a grayscale image can be mapped into a pixel grid which contains a number that gives a relative value of its brightness. Each pixel representing a number between 0 and 255 which gives the colour black and white respectively.
5
In the end, I had to think about making a neural network that is able to recreate my input into the 3 RGB colour grids.
I generate a training data when I modify already coloured input to grayscale, we get the formula of: L = (R+G+B) /3. But this is only considered to want a help in the saturations balancing and composition. I used a saturation algo to help me with the conversion of an RGB to a colour channel of LAB. L in this channel represents the lightness and a and b red-green and blue-yellow composition respectively.
In this context, I have not discarded the actual grayscale input I used and can also be utilised in our prediction as its quite sharper than other colour encoded pictures.
Now this sharpness of grayscale image is useful as it provides us with the values for the brightness/L components. All will be left is to compute the value of a and b to create a final LAB image.
This I did by adding filter in the Convolutional Neural Network. Filters in an ANN is used to analyse and extract corresponding data from the input images and are adjusted accordingly to generate a reliable outcome.
6
Since, I have written the entire code in python, I have made use of the Anaconda Navigator as my base(root) and the Spyder software as my IDE to run all my codes.
I mounted a neural dataset and applied it with the tensorflow package and modified the epoch value that helped me to visualize what’s the number of times the CNN learns from the input provided.
What I was aiming for was a fusion type colorization model that is able to combine and compute the different coloured phases that the neural network creates at varied epoch values and structure them all together to get the desired output.
The training data that I used here was from “ImageNet” which is a very famous location for image related processing.
So, the plan was that the true values of colour would be in the effective limit of +(-)128. As I was using the Lab space channel, which computes the effective chroma value by measuring the distance between the pixel and the origin, I took the LCM out as common and made the final values in the range of 1 and -1. It actually helped that the error of the prediction was
7
comparable. As it was a ConvNet, it could automatically modify the filters to accommodate the errors calculated.
So, after my neural network was done, I provided the grayscale image as input and selecting green.red and blue. yellow as two filter layers for filling the (a.b) spaces, I used the trained CNN to perform and provide a predicted output which we extracted.
Lastly, I added this 2 layered projected output into the earlier RGB sum of arrays. This array which contains pixel grids is converted as an output image. But not everything worked as predicted ad the code was only working for red and green specified value colours. The problem I found here was that activation function that I used in the LAB colour channel only mapped out non-negative values which resulted in half of the pixels in the wrong shad of colours. In order to rectify that, I used a Hue beta function that was able to include the blue.yellow colours and fixed the values.
The above image depicting the fusion which I mentioned earlier between 2 layers (here it’s the intermediary output and definite-dimension embedding image) which is an output produced by the CNN.
However, I was unable to input the decoder-encoder function that lets one to input the size of any image and the coloured image generated would be resized to match the original input dimensions.
8
2.2 The feature extractor
Another issue that rises here in this is that my network is unable to produce a decent colorization it is restricted in the number of pixels it receives to analyse and predict.
For example, this pixel here is depicted by a 3X3 grid which has very less information for my network to work on, as such my models fails to learn and produce outputs for certain image that have a limited number to work on in the first place(for eg. Historical images). So what I did was make the network extract the data by combining and overlapping images of the visual patterns to improvise the filters that I used. The Neural network actually functions or trial/error basis with making a random analysis and prediction and the using backpropagation to enhance the image with the feature extraction.
Fusing the Inception embedding with the output of the convolutional layers of the encoder
This feature extractor function can be used to determine high quality images like underwater and indoor types and be used in the colorization model. Here, I used the Inception model with two 16-VGG layers that is pretrained to help in the extraction process of image embedding.
9
2.3 Technical explanation
The real uncommonness in this model than those others is that I emphasized more on pixel relativity in the space. For the fact that image ration will be maintained continuously when being extracted in the network.
When we move an image that shows clearly on a small screen (for example a phone) to a larger screen, take a 4K television for example, what we notice is that the image is distorted as the pooling layers only works on the input values and not the entire layout that is showing on the small screen image. In this model what I did was that I maintained the width-eight ratio which increased the per pixel capability of holding data much more and did not result in a distorted output.
As I know form the anti-aliasing techniques of computer graphics, the up _ sampling leads to a reduced quality as the only remaining output is quantified and the rest is not taken in for consideration and there is a decrement in image quality.
This model on the other hand kept the image ratio constant, which I did with the addition of white padding.
I use another function for image output by getting more training dataset and extract a grayscale image and put it in one epoch while 2 different colours to make my other part of fusion layers.
10
These were the images I output after training the neural network with around 4200+ images. The effect was not as expected and, in some photos, it was underperforming because of their unique colour composition. The result of mapping them to range of -128 to 128 was leaving it with a slight brown tint and was vastly different form the ground truth not to mention the human eye.
The colour spaces that mainly came into picture here were the Lab that I used for image generation in the fusion layer structure and the YUV which were used in the Hypercolumns.
The neural network design here was reproduced with fusion semantics and the colorization network was in Keras which I considered to be optimum in combining multiple layers of convolutions.
I use the Tensorflow to input the weights into the edges of the neural network and use the black and white images to run through Keras in the backend.
Here, I just fixed and resized the image to the Inception model and then I made use of the pre-processor to rectify any errors in pixel or colour variables and lastly run the image through the model and finally extract the desired outcome which is the colorized image.
11
3. Results and Discussion:
After building the neural network, I trained it on different types of datasets. I even tried to flip and modify the same photos so that it can learn more on that set of images. Once trained, I gave some grayscale inputs and succeeded in generating a decent quality of colorized images that were having a bit less amount of saturation and the places where sunlight is not the primary source of radiance or underwater photos, I found the outputs to be less than subpar to that of the original photo. Though more improvements can be done my introducing some more layers in neural network and training the model to more number of datasets. But due to time restriction and hardware capability, I’ve been successful in producing a satisfactory result. Here below are some outputs and compared with the original phots in relative to the black and white input:
12
In the earlier attempts, grasses and greenery always have different saturation levels as compared to the original. Though places where a darker shade were more prominent are well colored.
I compared my own results to those who have worked previously on creating a colorization neural network model. Some of them have used the same dataset as mine to train their models and I’ve noticed that people who have used balancing classification structures have although produced a decent quality in some photos but a huge amount of them also have quite misappropriated saturation levels and have generated a rather gray image. For example, in the case of sunflowers most were yellow but depending on the light intensity and warmness level of the photograph the images weighted between blue green and yellow, so instead their model chooses and neutral zone and made it grayish.
Ri:chard Zhng has used a huge number of datasets in their models spanning over 1.2 million images and their approach at this colorization has produce results that outperforms other in a variety of field.
Comparing my models to others and reading and understanding their approach have given me more thoughts on my own model and means to improvise it in the areas where I actually though they are truly lacking. I think that improving and increasing the amount of dataset to train the model into more learning about the patterns is a good way to work on. By working more on these paths I am confident in being able to generate more close to the Realtime images that could help improve visual graphics in today’s world.
